{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47e5f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\cdc projects\\book_recommender_system\\brs\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4202d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fine_tuning_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0fd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"desc\":\"text\", \"simple_categories\":\"label_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e659fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Even if you know Hawaiian history you will fi...</td>\n",
       "      <td>Nonfiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the course of the long debate on the nature...</td>\n",
       "      <td>Nonfiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This engrossing book examines the particular i...</td>\n",
       "      <td>Nonfiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label_name\n",
       "0  \"Even if you know Hawaiian history you will fi...  Nonfiction\n",
       "1  In the course of the long debate on the nature...  Nonfiction\n",
       "2  This engrossing book examines the particular i...  Nonfiction"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7626317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to ID mapping: {'Nonfiction': 0, 'Fiction': 1}\n",
      "ID to Label mapping: {0: 'Nonfiction', 1: 'Fiction'}\n",
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label_name', 'label'],\n",
      "        num_rows: 3150\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label_name', 'label'],\n",
      "        num_rows: 788\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Create label mappings\n",
    "# This converts our text labels (\"Fiction\", \"Nonfiction\") into integers (0, 1)\n",
    "# which the model needs.\n",
    "labels = list(df['label_name'].unique())\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "print(f\"Label to ID mapping: {label2id}\")\n",
    "print(f\"ID to Label mapping: {id2label}\")\n",
    "\n",
    "# Apply the mapping to our dataframe\n",
    "df['label'] = df['label_name'].map(label2id)\n",
    "\n",
    "# Convert the pandas DataFrame to a Hugging Face Dataset object\n",
    "hg_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "train_test_split = hg_dataset.train_test_split(test_size=0.2)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n",
    "print(\"\\nDataset structure:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2019af5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3150/3150 [00:00<00:00, 6090.10 examples/s]\n",
      "Map: 100%|██████████| 788/788 [00:00<00:00, 7276.31 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3150\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 788\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"label_name\"])\n",
    "print(\"\\nTokenized dataset structure:\")\n",
    "print(tokenized_datasets)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    \n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38591ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-large-mnli and are newly initialized because the shapes did not match:\n",
      "- classification_head.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classification_head.out_proj.weight: found shape torch.Size([3, 1024]) in the checkpoint and torch.Size([2, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart-fiction-nonfiction-classifier\", \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",       \n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406675f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"\\nStarting fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete.\")\n",
    "\n",
    "print(\"\\nEvaluating final model on the test set:\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b90e997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
